{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoritical Questions**"
      ],
      "metadata": {
        "id": "1CungywGIHEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is a Decision Tree, and how does it work?\n",
        "- A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It works by splitting the data into subsets based on the value of input features, creating a tree-like model of decisions. Each internal node represents a feature (or attribute), each branch represents a decision rule, and each leaf node represents an outcome (or class label).\n",
        "\n",
        "The process of building a Decision Tree involves:\n",
        "\n",
        "Selecting the Best Feature: At each node, the algorithm selects the feature that best separates the data into classes based on a certain criterion (e.g., Gini impurity, entropy).\n",
        "Splitting the Data: The data is split into subsets based on the selected feature's value.\n",
        "Recursion: The process is repeated recursively for each subset until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf, or no further improvement).\n",
        "\n",
        "\n",
        "Q2. What are impurity measures in Decision Trees?\n",
        "- Impurity measures are metrics used to evaluate how well a feature separates the classes in a dataset. They help in determining the best feature to split the data at each node of the Decision Tree. Common impurity measures include:\n",
        "\n",
        "Gini Impurity: Measures the probability of misclassifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\n",
        "Entropy: Measures the amount of disorder or uncertainty in the dataset. It quantifies the unpredictability of the information content.\n",
        "Classification Error: The proportion of misclassified instances in a subset.\n",
        "\n",
        "Q3. What is the mathematical formula for Gini Impurity?\n",
        "- The Gini Impurity for a dataset is calculated using the formula:\n",
        "\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "( D ) is the dataset,\n",
        "( C ) is the number of classes,\n",
        "( p_i ) is the proportion of instances belonging to class ( i ) in the dataset.\n",
        "\n",
        "Q4. What is the mathematical formula for Entropy?\n",
        "- The Entropy of a dataset is calculated using the formula:\n",
        "\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "( D ) is the dataset,\n",
        "( C ) is the number of classes,\n",
        "( p_i ) is the proportion of instances belonging to class ( i ) in the dataset.\n",
        "\n",
        "Q5. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "- Information Gain is a measure of the effectiveness of an attribute in classifying the training data. It quantifies the reduction in entropy or impurity after a dataset is split on an attribute. The formula for Information Gain is:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "( IG(D, A) ) is the Information Gain of attribute ( A ) with respect to dataset ( D ),\n",
        "( Values(A) ) are the possible values of attribute ( A ),\n",
        "( D_v ) is the subset of ( D ) for which attribute ( A ) has value ( v ),\n",
        "( |D| ) is the total number of instances in dataset ( D ),\n",
        "( |D_v| ) is the number of instances in subset ( D_v ).\n",
        "In Decision Trees, Information Gain is used to select the attribute that provides the highest reduction in uncertainty (or impurity) when splitting the data, thus guiding the construction of the tree."
      ],
      "metadata": {
        "id": "sjlMGu2hINGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Difference between Gini Impurity and Entropy\n",
        "\n",
        "- Definition:\n",
        "\n",
        "Gini Impurity: Measures the probability of misclassifying a randomly chosen element from the dataset. It ranges from 0 (pure) to 0.5 (maximum impurity for binary classification).\n",
        "Entropy: Measures the amount of uncertainty or disorder in the dataset. It ranges from 0 (pure) to logâ‚‚(C) for multi-class problems, where C is the number of classes.\n",
        "Computational Complexity:\n",
        "\n",
        "Gini is computationally simpler and faster as it does not involve logarithmic calculations.\n",
        "Entropy requires logarithmic calculations, making it more computationally intensive.\n",
        "Bias:\n",
        "\n",
        "Gini tends to favor splits that result in a more balanced distribution of classes.\n",
        "Entropy is biased towards selecting splits that result in a higher reduction of uncertainty.\n",
        "Usage:\n",
        "\n",
        "Gini is commonly used in CART (Classification and Regression Trees).\n",
        "Entropy is used in algorithms like ID3 and C4.5.\n",
        "\n",
        "\n",
        "Q7. Mathematical Explanation behind Decision Trees\n",
        "\n",
        "- Structure:\n",
        "\n",
        "Decision Trees consist of nodes (decision nodes and leaf nodes) and edges (branches).\n",
        "Splitting Criteria:\n",
        "\n",
        "The tree is built by recursively splitting the dataset based on feature values to minimize impurity (Gini or Entropy).\n",
        "Information Gain:\n",
        "\n",
        "Information Gain is calculated as the difference in entropy before and after a split: [ IG(S, A) = H(S) - \\left( \\frac{|S_1|}{|S|} H(S_1) + \\frac{|S_2|}{|S|} H(S_2) \\right) ]\n",
        "Where (H(S)) is the entropy of the dataset before the split, and (H(S_1)) and (H(S_2)) are the entropies of the subsets after the split.\n",
        "\n",
        "Stopping Criteria:\n",
        "\n",
        "The splitting process stops when a node reaches a maximum depth, contains fewer than a minimum number of data points, or results in a pure node.\n",
        "\n",
        "\n",
        "Q8. Pre-Pruning in Decision Trees\n",
        "\n",
        "- Definition: Pre-Pruning, also known as early stopping, involves halting the growth of the decision tree during the training phase to prevent overfitting.\n",
        "\n",
        "Methods:\n",
        "\n",
        "Set a maximum depth for the tree.\n",
        "Specify a minimum number of samples required to split a node.\n",
        "Define a minimum number of samples per leaf.\n",
        "Limit the number of features considered for splitting.\n",
        "Purpose: The goal is to simplify the model and enhance generalization by avoiding overly complex trees.\n",
        "\n",
        "Q9. Post-Pruning in Decision Trees\n",
        "\n",
        "- Definition: Post-Pruning involves allowing the decision tree to grow fully and then removing branches that do not contribute significantly to predictive accuracy.\n",
        "\n",
        "Methods:\n",
        "\n",
        "Cost-Complexity Pruning: Assigns a cost to subtrees based on their complexity and selects the subtree with the smallest cost.\n",
        "Reduced Error Pruning: Removes branches that do not improve validation accuracy.\n",
        "Minimum Impurity Decrease: Prunes nodes if the reduction in impurity is below a certain threshold.\n",
        "Purpose: This technique aims to reduce overfitting and improve the model's performance on unseen data.\n",
        "\n",
        "Q10. Difference between Pre-Pruning and Post-Pruning\n",
        "\n",
        "- Timing:\n",
        "\n",
        "Pre-Pruning occurs during the tree construction process, stopping growth based on predefined criteria.\n",
        "Post-Pruning takes place after the tree has been fully constructed, where unnecessary branches are removed.\n",
        "Approach:\n",
        "\n",
        "Pre-Pruning aims to prevent the tree from becoming too complex from the outset.\n",
        "Post-Pruning focuses on refining the tree after it has been built to enhance its predictive power.\n",
        "Impact on Model:\n",
        "\n",
        "Pre-Pruning can lead to a simpler model that may underfit if too restrictive.\n",
        "Post-Pruning can help in retaining a more complex model while reducing overfitting"
      ],
      "metadata": {
        "id": "Yvc-1rIRI0Sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What is a Decision Tree Regressor?\n",
        "- A Decision Tree Regressor is a machine learning model that predicts continuous outcomes by learning decision rules from the features of the dataset. It operates by recursively splitting the data into subsets based on feature values, ultimately forming a tree-like structure where each leaf node represents a predicted value.\n",
        "\n",
        "Q12. What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "- Advantages:\n",
        "\n",
        "Easy to Understand: Decision trees are intuitive and can be visualized easily, making them accessible for non-technical stakeholders.\n",
        "Handles Non-Linear Relationships: They can model complex relationships between features and target variables effectively.\n",
        "Minimal Data Preparation: Decision trees require little preprocessing, such as scaling or normalization.\n",
        "Robust to Outliers: They are generally insensitive to outliers, as extreme values do not significantly affect the splits.\n",
        "Automatic Feature Selection: Decision trees can identify and prioritize important features during the splitting process.\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Overfitting: They are prone to overfitting, especially with deep trees, which can lead to poor generalization on unseen data.\n",
        "\n",
        "Instability: Small changes in the data can result in different tree structures, making them less stable.\n",
        "\n",
        "Bias Towards Features with More Levels: Decision trees may favor features with many categories, potentially leading to misleading results.\n",
        "\n",
        "Limited Expressiveness: They may struggle to capture certain complex relationships compared to more advanced models like neural networks.\n",
        "\n",
        "Q13. How does a Decision Tree handle missing values?\n",
        "- Decision Trees can handle missing values in several ways:\n",
        "\n",
        "Surrogate Splits: They can create alternative splits based on other features when a value is missing.\n",
        "\n",
        "Imputation: Missing values can be filled in with the most common value or the mean/median of the feature.\n",
        "\n",
        "Assigning Probabilities: Decision trees can assign missing values based on the probabilities derived from other samples, ensuring that the model remains robust despite incomplete data.\n",
        "\n",
        "Q14. How does a Decision Tree handle categorical features?\n",
        "- Decision Trees handle categorical features by splitting the data based on the categories of the feature. Each category can lead to a different branch in the tree, allowing the model to make decisions based on the presence or absence of specific categories. This capability makes Decision Trees particularly effective for datasets with mixed data types.\n",
        "\n",
        "Q15. What are some real-world applications of Decision Trees?\n",
        "- Decision Trees are widely used in various fields, including:\n",
        "\n",
        "Customer Segmentation: Identifying distinct customer groups based on purchasing behavior.\n",
        "\n",
        "Credit Scoring: Assessing the creditworthiness of individuals by analyzing their financial history.\n",
        "\n",
        "Medical Diagnosis: Classifying patients based on symptoms and medical history to predict diseases.\n",
        "\n",
        "Predicting Housing Prices: Estimating property values based on features like location, size, and amenities.\n",
        "\n",
        "Risk Assessment: Evaluating potential risks in finance, insurance, and project management"
      ],
      "metadata": {
        "id": "YEOpETzbJ_nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESTIONS**"
      ],
      "metadata": {
        "id": "7ikq2kB8S_iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "4geWeeP4TGMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train classifier with Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature importances (Gini):\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZQ9V4F-qTOwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train classifier with entropy\n",
        "clf = DecisionTreeClassifier(criterion='entropy')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy (Entropy): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "afuWljXmTVP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Boston housing data\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "91MXp4j6TbTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Export tree in DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize using graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_decision_tree\")  # saves as PDF file\n",
        "graph.view()  # opens the PDF file in default viewer\n"
      ],
      "metadata": {
        "id": "yyroOv5qTiWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Fully grown tree (default max_depth=None)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy with max_depth=3: {acc_limited:.4f}\")\n",
        "print(f\"Accuracy with fully grown tree: {acc_full:.4f}\")\n"
      ],
      "metadata": {
        "id": "BZkLn2oRTneH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree\n",
        "\n",
        "# Using the same iris dataset and train/test split from above\n",
        "\n",
        "# Tree with min_samples_split=5\n",
        "tree_min_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "tree_min_split.fit(X_train, y_train)\n",
        "y_pred_min_split = tree_min_split.predict(X_test)\n",
        "acc_min_split = accuracy_score(y_test, y_pred_min_split)\n",
        "\n",
        "# Default tree\n",
        "tree_default = DecisionTreeClassifier(random_state=42)\n",
        "tree_default.fit(X_train, y_train)\n",
        "y_pred_default = tree_default.predict(X_test)\n",
        "acc_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "print(f\"Accuracy with min_samples_split=5: {acc_min_split:.4f}\")\n",
        "print(f\"Accuracy with default min_samples_split: {acc_default:.4f}\")\n"
      ],
      "metadata": {
        "id": "fjj6uR5kTtVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Unscaled data tree\n",
        "tree_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "tree_unscaled.fit(X_train, y_train)\n",
        "acc_unscaled = accuracy_score(y_test, tree_unscaled.predict(X_test))\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Tree on scaled data\n",
        "tree_scaled = DecisionTreeClassifier(random_state=42)\n",
        "tree_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, tree_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy on unscaled data: {acc_unscaled:.4f}\")\n",
        "print(f\"Accuracy on scaled data: {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "fWVsnHQlT0g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# OvR wrapped decision tree\n",
        "ovr_tree = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr_tree.fit(X_train, y_train)\n",
        "y_pred_ovr = ovr_tree.predict(X_test)\n",
        "acc_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "print(f\"Accuracy using One-vs-Rest Decision Tree: {acc_ovr:.4f}\")\n"
      ],
      "metadata": {
        "id": "QnHabwnNT6yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "\n",
        "# Train tree\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "importances = tree.feature_importances_\n",
        "for name, score in zip(data.feature_names, importances):\n",
        "    print(f\"{name}: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "bou6oY-_UA12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset (Boston housing)\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Decision Tree Regressor with max_depth=5\n",
        "tree_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "\n",
        "# Unrestricted Decision Tree Regressor\n",
        "tree_unrestricted = DecisionTreeRegressor(random_state=42)\n",
        "tree_unrestricted.fit(X_train, y_train)\n",
        "y_pred_unrestricted = tree_unrestricted.predict(X_test)\n",
        "\n",
        "# Evaluate with MSE\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "print(f\"MSE with max_depth=5: {mse_limited:.3f}\")\n",
        "print(f\"MSE unrestricted tree: {mse_unrestricted:.3f}\")\n"
      ],
      "metadata": {
        "id": "1KE_Sf3jUFVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train initial tree to get ccp_alphas\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_scores.append(clf.score(X_train, y_train))\n",
        "    test_scores.append(clf.score(X_test, y_test))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(ccp_alphas, train_scores, marker='o', label='Train Accuracy')\n",
        "plt.plot(ccp_alphas, test_scores, marker='o', label='Test Accuracy')\n",
        "plt.xlabel('ccp_alpha')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of CCP Alpha on Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "itllLaAFUL5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "FZZ3fToWUR2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KJP-ten9UZRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_clf = grid_search.best_estimator_\n",
        "print(\"Test set accuracy:\", best_clf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "45ih94jpUfH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}